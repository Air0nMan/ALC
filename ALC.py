# -*- coding: utf-8 -*-
"""py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XM63xWkLvCpbsO-PjwO0bx87qVXXaGz-
"""

import numpy as np
import math

def esCuadrada(matrizInput):
    if(len(matrizInput[0]) == len(matrizInput)):
        return True
    else:
        return False


def triangSup(A): #devuelve la matriz A transformada en una matriz triangular superior mediante eliminacion gaussiana
    A = A.copy().astype(float)
    n, m = A.shape # dimensiones de A
    k = min(n, m)   # por si la matriz no es cuadrada

    for i in range(k):

        # 1. Pivoteo parcial: buscar fila con el mayor valor absoluto en la columna i
        if A[i, i] == 0:
            for j in range(i + 1, n):
                if A[j, i] != 0:
                    A[[i, j]] = A[[j, i]]  # swap de filas
                    break

        # Si sigue siendo 0, no se puede eliminar en esta columna
        if A[i, i] == 0:
            continue

        # 2. Eliminación hacia abajo
        for z in range(i + 1, n):
            factor = A[z, i] / A[i, i]
            A[z] = A[z] - factor * A[i]

    return A


def rango(A): #devuelve el rango de la matriz A
    B = triangSup(A) # primero la triangulo
    n,m= B.shape # dimensiones de B
    cont = 0 #contador de filas no nulas
    for i in range(n): #recorro filas y cuento las que no son nulas
        vector_nulo = 0
        for j in range(m):
            if B[i][j] != 0:
                continue
            else:
                vector_nulo+=1
        if vector_nulo < m:
            cont+=1   
    return cont




def diagonal(A): #devuelve una matriz que contiene solamente la diagonal de la matriz original A, y 0 en las demas posiciones
    res = np.zeros(A.shape)
    for i in range (A.shape[0]):
        res[i,i] = A[i,i]
    return res

def vectorDiagonal(A): #dado un vector de tamaño n, devuelve una matriz nxn con los elementos del vector en la diagonal y cero en las demas posiciones
    n=len(A)
    res = np.zeros((n,n))
    for i in range(n):
        res[i,i]=A[i]


def intercambiarFilas(A,i,j): # intercambia las filas i y j de la matriz A
    A=np.array(A)
    A[[i,j]] = A[[j,i]]
    return A


def sumarFilaMultiplo(A,i,j,s):
    A= np.array(A)
    A[i]+= s*A[j]
    return A


def traza(A): #devuelve la traza de la matriz A
    res = 0
    for i in range (len(A)):
        res+= A[i][i]
    return res




def calcularAx (A, x): # calcula el producto Ax donde A es una matriz y x un vector
    """res = []
    n = len(A)
    m = len(x)

    for i in range(n):
        suma = 0
        for j in range(m):
            suma += A[i][j]*x[j]
        res.append(suma)
    return np.array(res)"""
    A = np.asarray(A, dtype=float)
    x = np.asarray(x, dtype=float)
    # multiplicación matriz–vector
    return A @ x



def esDiagonalmenteDominante(A): #verifica si la matriz A es diagonalmente dominante
    for i in range (len(A)):
        suma = 0
        diagonal = 0
        for j in range (len(A[0])):
            if i == j:
                diagonal = abs(A[i][j])
            else:
                suma+= abs(A[i][j])
        if diagonal < suma:
            return False
    return True


def matrizVandermonde(v):
    res = []
    for i in range(len(v)):
        res.append([])
        for j in range(len(v)):
            res[i].append(v[i]**j)
    return np.array(traspuesta(res))



def producto_externo(x,y): # calcula el producto externo de dos vectores x e y
    """A = np.zeros((len(x),len(y)))

    for i in range(len(x)):
        for j in range(len(y)):
            A[i][j] = x[i] * y[j]
    return A"""
    x = np.asarray(x, dtype=float).reshape(-1, 1)   # vector columna
    y = np.asarray(y, dtype=float).reshape(1, -1)   # vector fila
    return x * y

def multiplicar_matrices(A, B): # calcula el producto de dos matrices A y B
    """m, n = A.shape
    n1, p = B.shape
    if n != n1:
      return None
    C = np.zeros((m, p))

    for i in range(m):
        for j in range(p):
            C[i,j] = multiplicar_vectores(A[i, :], B[:, j]) """
    A = np.asarray(A, dtype=float)
    B = np.asarray(B, dtype=float)
    C = A@B
    return C

def traspuesta(A): #devuelve la traspuesta de la matriz A
    """n,m = A.shape # dimensiones de A
    res = np.zeros((m,n)) # creo una matriz de dimensiones invertidas
    for i in range(m):
        for j in range(n):
            res[i][j] = A[j][i] # asigno los valores traspuestos
    return res"""
    A = np.asarray(A, dtype=float)
    return np.array(list(zip(*A)))

def esSimetrica(A,atol=0): #verifica si la matriz A es simetrica con una tolerancia atol
  matriz = np.array(A)
  if matriz.ndim !=2:
   return False
  filas, columnas = matriz.shape # dimensiones de la matriz
  if filas!=columnas: # una matriz no cuadrada no puede ser simetrica
   return False
  for i in range(filas):
   for j in range(filas):
    if error_relativo(matriz[i][j], matriz[j][i])>atol: # comparo elemento a elemento con la tolerancia
     return False
  return True

#Labo 1
def error(x,y): # calcula el error absoluto entre dos valores x e y
  x= np.float64(x)
  y= np.float64(y)
  return np.abs(x-y)


def error_relativo(x,y): # calcula el error relativo entre dos valores x e y
  x=np.float64(x)
  y=np.float64(y)

  if x==0:
    return np.inf
  else:
      return np.abs(x-y)/np.abs(x)


def matricesiguales (A,B,tol=0): #verifica si dos matrices A y B son iguales con una tolerancia tol
  if A.shape!= B.shape: # Si las matrices tienen dimensiones distintas, no son iguales
    return False
  for i in range (A.shape[0]):
      for j in range (A.shape[1]):
        if error_relativo(A[i][j],B[i][j]) > tol: # comparo elemento a elemento con la tolerancia
          return False
  return True

#labo2

def rota(theta): #devuelve la matriz de rotacion de angulo theta
  cos_t=np.cos(theta)
  sen_t=np.sin(theta)
  return np.array([[cos_t, -sen_t],[sen_t,  cos_t]])

def escala(s):
  s= np.array(s)
  n=len(s)
  matriz= np.zeros((n,n))
  for i in range (n):
    matriz[i][i]= s[i]
  return matriz

def rotayescala(theta,s):
    res = rota(theta)
    res[0] = res[0]*s[0]
    return np.array(res)

#labo3

def norma(x,p): # calcula la norma p de un vector x
    if(p == "inf"): #se rompe en lista vacia
        max = abs(x[0])
        for i in x:
           if(max < abs(i)):
               max = abs(i)
        return max

    sum = 0
    for i in x:
        sum += abs(i)**p # suma de las potencias
    sum = sum ** (1/p) # raiz p de la suma
    return sum

def normaliza(X, p ): # normaliza cada fila de la matriz X segun la norma p
    res = [None]*len(X)
    for i in range(0,len(X)):
        res[i] = [None]*len(X[i])
        for j in range (len(X[i])):
            res[i][j] = X[i][j]/norma(X[i],p) # normalizo cada fila
    return res

def normaExacta(a,p=[1,"inf"]): 
    if p == 2:
        return None

    cantFilas = len(a)
    CantColumnas = len(a[0])

    resinf = 0
    for i in range(0,cantFilas):
        sum = 0
        for j in range(0, CantColumnas):
            sum += abs(a[i][j])
        if sum > resinf:
            resinf = sum

    res1 = 0
    for j in range(0,CantColumnas):
        sum = 0
        for i in range(0, cantFilas):
            sum += abs(a[i][j])
        if sum > res1:
            res1 = sum
    if p == 1:
      return res1
    elif p == "inf":
      return resinf
    return res1,resinf


def _generar_vector_normalizado_p(m, p): # genera un vector random de tamaño m normalizado con la norma p

    if p == 2:
        x = np.random.normal(size = m)
    else:
        x = np.random.uniform(-1, 1, m)
    norm_x = norma(x, p)
    if norm_x == 0:
        return np.zeros(m)
    return x / norm_x

def normaMatMC(A, q, p, Np): # calcula la norma matricial usando Monte Carlo con Np vectores aleatorios
    res = 0
    vector_max = None

    A_mat = np.array(A)
    n, m = np.shape(A_mat) # dimensiones de A

    for i in range(Np): # genero Np vectores aleatorios normalizados
        x_norm = _generar_vector_normalizado_p(m, p)
        Ax = calcularAx(A_mat, x_norm)
        normaX = norma(Ax, q) # calculo la norma del vector Ax

        if normaX > res: # guardo el maximo
            res = normaX
            vector_max = x_norm

    return res, vector_max

def condExacta(A,p): # calcula el numero de condicion exacto de la matriz A con la norma p
    inv = inversa(A) # calculo la inversa de A
    res = 0
    if p == 1:
        res = normaExacta(A,p)[0]*normaExacta(inv)[0]
    elif p == "inf":
        res = normaExacta(A,p)[1]*normaExacta(inv)[1]
    else: 
        res = normaMatMC(A,p,p,10000)[0]* normaMatMC(inv,p,p,10000)[0]

    return res

def condMC( A, p ): # calcula el numero de condicion aproximado de la matriz A con la norma p usando Monte Carlo
    inv = inversa(A)
    res = 0
    res = normaMatMC(A,p,p,10000)[0]* normaMatMC(inv,p,p,10000)[0]
    return res

#labo4
def calculaLU(M): # calcula la descomposicion LU de la matriz M
    if M is None or M.ndim != 2 or M.shape[0] != M.shape[1]: # verifico que M sea cuadrada
        return None, None, 0
    M = M.copy().astype(float)
    N = M.shape[0]

    U = M.copy()
    L = np.eye(N)

    contador = 1
    for i in range(N):
        pivot = U[i,i]
        if pivot == 0:
            return None,None,0
        for j in range(i + 1, N):
            coeficiente = U[j, i] / pivot
            U[j, :] -= coeficiente * U[i, :]
            L[j, i] = coeficiente
            contador+=4

    return L,U, contador

def res_tri(L,b, inferior = True): # resuelve el sistema Lx=b si inferior es True, o Ux=b si inferior es False
    n = len(L)
    x = [0.0] * n
    if inferior: # sistema triangular inferior
        for i in range( n ):
            suma = 0.0
            for j in range( i ):
                suma += L[i][j] * x[j]
            x[i] = ( b[i] - suma ) / L[i][i]
    else: # sistema triangular superior
        for i in range(n - 1, -1, -1):
            suma = 0.0
            for j in range(i+1, n):
                suma += L[i][j] * x[j]
            x[i] = (b[i] - suma) / L[i][i]
    return x

def inversa(M): # calcula la inversa de la matriz M usando la descomposicion LU
    n = M.shape[0] # dimensiones de M
    L, U, T = calculaLU(M) # calculo la descomposicion LU de M
    inversa = np.array([])
    if L is None:  # si no se pudo calcular la descomposicion LU, devuelvo None
        inversa = None
    else:
        inversa = np.zeros((n, n))
        I = np.eye(n)
        for i in range(n):
            e = I[:, i]
            y = res_tri(L, e, True)
            x = res_tri(U, y, False)
            inversa[:, i] = x

    return inversa

def calculaLDV (A): # calcula la descomposicion LDV de la matriz A
  L,U,_ = calculaLU(A) # primero calculo la descomposicion LU de A
  if (L is None or U is None): # si no se pudo calcular la descomposicion LU, devuelvo None
   return None,None,None
  V,D,_ = calculaLU(traspuesta(U)) #calculo la descomposicion LU de la traspuesta de U
  if (D is None or V is None):# si no se pudo calcular la descomposicion LU, devuelvo None
     return None,None,None
  return L,D,traspuesta(V)

def cholesky(A):# calcula la descomposicion de Cholesky de la matriz A asumiendo que cumple las condiciones necesarias
    L,U,_ = calculaLU(A) # primero calculo la descomposicion LU de A
    D = diagonal(U) # me queda con la diagonal de U
    for i in range(L.shape[0]):
        L[:,i] = L[:,i] * np.sqrt(D[i,i])
    return L,traspuesta(L)
    

def esSDP(A,atol=1e-8):# verifica si la matriz A es simetrica definida positiva con una tolerancia atol
  if A is None:
    return None
  if not esSimetrica(A,atol):# verifica si A es simetrica
    return False
  L,D,V = calculaLDV(A)# calcula la descomposicion LDV de A
  if D is None:
    return None
  n=A.shape[0]
  for i in range(n):
    if D[i][i] <= atol :# verifica si los elementos de la diagonal de D son positivos
      return False

  return True

def multiplicar_vectores(x,y):# calcula el producto "." entre dos vectores a y b
  """if len(a)!=len(b): # verifico que los vectores tengan la misma dimension
    return None
  res = 0
  for i in range (len(a)):
    res+=a[i]*b[i]
  return res"""
  x = np.asarray(x, dtype=float)
  y = np.asarray(y, dtype=float)
  return np.sum(x * y)

def QR_con_GS(A, tol=1e-12,retorna_nops=False): # descomposicion QR usando el metodo de Gram-Schmidt
    m, n = A.shape # dimensiones de A
    Q = np.zeros((m, n))
    R = np.zeros((n, n))

    r = 0  # contador de columnas independientes

    for j in range(n):
        # Copiamos la j-ésima columna de A
        v = A[:, j].copy()

        # Proyección sobre las columnas anteriores de Q válidas
        for k in range(r):
            R[k, j] = multiplicar_vectores(Q[:, k], v)
            v -= R[k, j] * Q[:, k]

        # Norma de la componente ortogonal
        R[r, j] = norma(v, 2)

        # Verificar si la norma es suficientemente grande
        if R[r, j] > tol:
            # Guardamos q_r y avanzamos el contador
            Q[:, r] = v / R[r, j]
            r += 1
        # Si no, el vector se descarta (columna dependiente)

    # Construimos matrices finales truncadas
    Q_hat = Q[:, :r]
    R_hat = R[:r, :]

    return Q_hat, R_hat

def QR_con_HH(A, tol=1e-12): # descomposicion QR usando Householder
    A = np.array(A, dtype=float)
    m, n = A.shape # dimensiones de A
    if m < n: # no se puede hacer la descomposicion QR
        return None

    R = A.copy()
    Q = np.eye(m)
    print(n)
    for k in range(n):
        print(k)
        x = R[k:, k]
        alfa = -np.sign(x[0]) * norma(x,2)
        e1 = np.zeros_like(x)
        e1[0] = 1
        u = x - alfa * e1

        if norma(u,2) > tol: # evito divisiones por cero
            u = u / norma(u,2)

            Hk = np.eye(m-k)-2*producto_externo(u, u)
            Hk_sub = np.zeros((m,m))
            for i in range(k):
                Hk_sub[i,i] = 1
            Hk_sub[k:,k:] = Hk
            

            R = multiplicar_matrices(Hk_sub, R)
            Q = multiplicar_matrices(Q,Hk_sub.T)

    return Q, R


def calculaQR(A,metodo='RH',tol=1e-12): # calcula la descomposicion QR de la matriz A usando el metodo pedido
  if metodo == "RH":
    return QR_con_HH(A,tol)
  elif metodo == "GM":
    return QR_con_GS(A,tol)
  return None

def metpot2k(A, tol=1e-12, K=1000):
    x = len(A[0])
    v = np.random.randn(x)
    v = v / norma(v, 2)

    wa = calcularAx(A, v)
    aval_viejo = 0
    aval_nuevo = multiplicar_vectores(v, wa)
    k = 0

    while k < K and abs(aval_nuevo - aval_viejo) > tol:
        v = wa / norma(wa, 2)
        wa = calcularAx(A, v)
        aval_viejo = aval_nuevo
        aval_nuevo = multiplicar_vectores(v, wa)
        k += 1

    return v, aval_nuevo, k

def diagRH(A, tol=1e-15, K=100, n_total=None):
    """
    Diagonaliza por Householder recursivo usando método de potencias.
    Además imprime un progreso aproximado en función del tamaño actual n.
    """
    A = np.asarray(A, dtype=float)
    n, _ = A.shape

    # En la primera llamada guardamos el tamaño total
    if n_total is None:
        n_total = n

    # Progreso aproximado: cuando n pasa de n_total a 1
    if n_total > 1:
        progreso = 100 * (n_total - n) / (n_total - 1)
        print(f"[diagRH] Progreso aproximado: {progreso:.2f}% (n = {n} de {n_total})")

    # Caso base n == 1
    if n == 1:
        return np.eye(1), np.array([[A[0, 0]]])

    # Autovector y autovalor dominante con método de potencias
    v, l, _ = metpot2k(A, tol=tol, K=K)   # v: autovector, l: autovalor

    # Construimos Householder H que lleva v a e1
    e1 = np.zeros(n)
    e1[0] = 1.0
    u = e1 - v
    denom = norma(u, 2)**2
    if denom == 0:   # por si v ≈ e1
        H = np.eye(n)
    else:
        # H = I - 2 u u^T / (u^T u)
        H = np.eye(n) - 2.0 * producto_externo(u, u) / denom

    if n == 2:
        S = H.copy()
        # D = H A H^T
        D = multiplicar_matrices(multiplicar_matrices(H,A),traspuesta(H))
        return S, D

    # Paso recursivo
    # B = H A H^T
    B = multiplicar_matrices(multiplicar_matrices(H,A),traspuesta(H))

    # Tomamos el bloque (n-1)x(n-1) de abajo a la derecha
    A1 = B[1:, 1:]          # slicing en lugar de doble for

    # Diagonalizamos recursivamente A1
    S1, D1 = diagRH(A1, tol=tol, K=K, n_total=n_total)

    # Armamos D completa: l en la primer diagonal, luego diag(D1)
    D = np.zeros_like(A)
    D[0, 0] = l
    D[1:, 1:] = D1

    # Armamos S completa: arriba-izquierda 1, el resto S1
    S = np.eye(n)
    S[1:, 1:] = S1

    # Aplicamos la transformación global
    S = multiplicar_matrices(H,S)

    return S, D

#labo7

def transiciones_al_azar_continuas(n):
    M = np.random.rand(n, n)
    suma_columnas = M.sum(axis=0)
    M = M / suma_columnas
    return M

def transiciones_al_azar_uniformes(n, thres):
    M = np.random.rand(n, n)
    M[M < thres] = 1.0
    M[M >= thres] = 0.0

    for j in range(n):
        if M[:, j].sum() == 0:
            M[0, j] = 1

    M = M / M.sum(axis=0)

    return M

def nucleo(A, tol=1e-15): # calcula el nucleo de la matriz A
    A_t = traspuesta(A)
    M = multiplicar_matrices(A_t, A)

    S, D = diagRH(M, tol=tol)

    n_diag = D.shape[0]
    autovalores = np.zeros(n_diag)
    for i in range(n_diag):
        autovalores[i] = D[i, i]
    autovalores = np.abs(autovalores) < tol
    autovectores_nucleo = S[:, autovalores]

    if autovectores_nucleo.shape[1] == 0:
        return np.zeros((0, 0))
    else:
        return autovectores_nucleo

def crea_rala(listado, m_filas, n_columnas, tol=1e-15):

    if len(listado) == 0:
        return {}, (m_filas, n_columnas)

    filas    = listado[0]
    columnas = listado[1]
    valores  = listado[2]

    dic = {}

    cantidad = len(filas)

    for k in range(cantidad):
        i   = filas[k]
        j   = columnas[k]
        aij = valores[k]

        if abs(aij) > tol:
            dic[(i, j)] = aij

    return [dic, (m_filas, n_columnas)]

def multiplica_rala_vector(A, v):
    dic = A[0]
    m_filas, n_columnas = A[1]
    res = np.zeros(m_filas)
    for (i, j), Aij in dic.items():
       res[i] += Aij * v[j]
    return res

def svd_reducida(A, k="max", tol=1e-15): # calcula la SVD reducida de la matriz A
    A = np.array(A, dtype=float)
    m, n = A.shape

    if m < n:
        print("[svd_reducida] Caso m < n, llamando recursivamente con A^T")
        U_t, S_vals, V_t = svd_reducida(traspuesta(A), k=k, tol=tol)
        hatV = U_t
        r = len(S_vals)

        if r == 0:
            hatU = np.zeros((m, 0))
            return hatU, S_vals, hatV

        hatU = np.zeros((m, r))
        for j in range(r):
            vj = hatV[:, j]
            Av = calcularAx(A, vj)
            hatU[:, j] = Av / S_vals[j]

        print("[svd_reducida] Terminado caso m < n")
        return hatU, S_vals, hatV

    print("[svd_reducida] Construyendo A^T A...")
    AT = traspuesta(A)
    AtA = multiplicar_matrices(AT, A)

    AtA = np.array(AtA, dtype=float)
    AtA = 0.5 * (AtA + traspuesta(AtA))

    print("[svd_reducida] Llamando a diagRH...")
    S, D = diagRH(AtA, tol=tol, K=10000)
    print("[svd_reducida] diagRH terminado, armando U, S, V...")

    S = np.array(S, dtype=float)
    D = np.array(D, dtype=float)

    autovalores = np.diag(D).copy()
    orden = np.argsort(-autovalores)
    autovalores = autovalores[orden]
    V = S[:, orden]
    autovalores[autovalores < 0] = 0.0

    indices_validos = autovalores > tol
    autovalores_validos = autovalores[indices_validos]
    V_validas = V[:, indices_validos]

    sigmas_completos = np.sqrt(autovalores_validos)
    r_total = len(sigmas_completos)

    if r_total == 0:
        hatU = np.zeros((m, 0))
        hatV = np.zeros((n, 0))
        print("[svd_reducida] Rango ~0, devolviendo vacíos")
        return hatU, sigmas_completos, hatV

    if k == "max":
        r = r_total
    else:
        r = min(k, r_total)

    hatS = sigmas_completos[:r]
    hatV = V_validas[:, :r]

    hatU = np.zeros((m, r))
    for j in range(r):
        vj = hatV[:, j]
        Av = calcularAx(A, vj)
        hatU[:, j] = Av / hatS[j]

    print("[svd_reducida] Terminado SVD reducida.")
    return hatU, hatS, hatV


#Funciones TP ALC

def cargarDataset(carpeta): # carga el dataset de gatos y perros desde la carpeta especificada

    X_train_cats = np.load(carpeta + "template-alumnos/cats_and_dogs/train/cats/efficientnet_b3_embeddings.npy")
    X_train_dogs = np.load(carpeta + "template-alumnos/cats_and_dogs/train/dogs/efficientnet_b3_embeddings.npy")

    Xt = np.concatenate((X_train_cats, X_train_dogs), axis=1)

    n_train_cats = X_train_cats.shape[1]
    n_train_dogs = X_train_dogs.shape[1]
    n_train_total = n_train_cats + n_train_dogs

    Yt = np.zeros((2, n_train_total))
    Yt[0, :n_train_cats] = 1 
    Yt[1, n_train_cats:] = 1  

    X_val_cats = np.load(carpeta + "template-alumnos/cats_and_dogs/val/cats/efficientnet_b3_embeddings.npy")
    X_val_dogs = np.load(carpeta + "template-alumnos/cats_and_dogs/val/dogs/efficientnet_b3_embeddings.npy")

    Xv = np.concatenate((X_val_cats, X_val_dogs), axis=1)

    n_val_cats = X_val_cats.shape[1]
    n_val_dogs = X_val_dogs.shape[1]
    n_val_total = n_val_cats + n_val_dogs

    Yv = np.zeros((2, n_val_total))
    Yv[0, :n_val_cats] = 1
    Yv[1, n_val_cats:] = 1

    return Xt, Yt, Xv, Yv

def pinvEcuacionesNormales(X,L, Y):
    ranX = rango(X)
    n,p = X.shape
    Lt = traspuesta(L)
    Xt = traspuesta(X)
    Yt = traspuesta(Y)
    if ranX == p and n>p:
        Z = np.zeros((p,n))
        for i in range(n):
            Z[:,i] = res_tri(L,Xt[:,i], inferior = True)
        U = np.zeros((p,n))
        for i in range(n):
            U[:,i] = res_tri(Lt,Z[:,i], inferior = False)
        W = multiplicar_matrices(Y,U)
    elif ranX == n:
        if n < p:
            Z = np.zeros((n,p))
            for i in range(p):
                Z[:,i] = res_tri(L,X[:,i], inferior = True)
            Vt = np.zeros((n,p))
            for i in range(p):
                Vt[:,i] = res_tri(Lt,Z[:,i], inferior = False)
            V = traspuesta(Vt)
            W = multiplicar_matrices(Y,V)
        elif n == p:
            Wt = np.zeros((n,Y.shape[0]))
            for i in range(Y.shape[0]):
                Wt[:,i] = res_tri(Xt,Yt[:,i], inferior = False)
            W = traspuesta(Wt)
    return W


def pinvSVD(U,S,V,Y):
 S_inv= np.diag((1/S))
 V1 = V[:, :len(S)] 
 X_pinv= V1 @ S_inv @ traspuesta(U)
 W= Y@ X_pinv
 return W


def pinvHouseHolder(Q, R, Y):
    n,p=Q.shape
    Q = Q[:,:n]
    R = R[:n,:]
    Qt = traspuesta(Q)
    Vt = np.zeros((p,n))
    print(n)
    for i in range(n):
        Vt[:,i] = res_tri(R,Qt[:,i], inferior = False)
        print(i)
    W = multiplicar_matrices(Y,traspuesta(Vt))
    return W

def pinvGramSchmidt(Q, R, Y):
    n,p=Q.shape
    Qt = traspuesta(Q)
    Vt = np.zeros((p,n))
    print(n)
    for i in range(n):
        Vt[:,i] = res_tri(R,Qt[:,i], inferior = False)
        print(i)
    W = multiplicar_matrices(Y,traspuesta(Vt))
    return W

def esPseudoInversa(X, pX, tol=1e-8):
    #Calculo las matrices que voy a necesitar para verifiar de antemano para ahorrar recursos
    XpX = multiplicar_matrices(X,pX)
    pXX = multiplicar_matrices(pX,X)
    
    
    if not matricesiguales(multiplicar_matrices(XpX,X),X,tol):
        return False
    elif not matricesiguales(multiplicar_matrices(pX,XpX),pX,tol):
        return False
    elif not matricesiguales(traspuesta(XpX),XpX,tol):
        return False
    elif not matricesiguales(traspuesta(pXX),pXX):
        return False
    return True
